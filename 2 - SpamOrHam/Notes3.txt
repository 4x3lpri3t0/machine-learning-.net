==============
Bayes’ Theorem
==============

P(A | B) = P(A)

* Knowing anything about B doesn’t convey any useful information about A.

* If A and B are not independent, Bayes’ Theorem would capture
how strongly the two pieces of information are related.

=======================
Dealing with Rare Words
=======================

If you pick a word that is present in only one of the categories (like “uncle” in Ham)
-> if we saw a message containing the word “uncle,” we would automatically
decide, with 100% confidence, that this message cannot be Spam, regardless of the rest of the message.
-> this can’t be right; we shouldn’t have 100% confidence in anything based on a limited training sample.

One way to address this problem is by using Laplace Smoothing.
-> technique that can be described as a “fudge factor.”

We will eliminate the problem of absent words by augmenting the count of each word by one,
and computing P(Spam contains “xyz”) = (1 + count of Spam containing “xyz”) / (1 + count of Spam)

Essentially, we are making up a fictional message containing every token
	-> rarest tokens will have a very low probability, but not zero.

========================
Combining Multiple Words
========================

