==============
Bayes’ Theorem
==============

P(A | B) = P(A)

* Knowing anything about B doesn’t convey any useful information about A.

* If A and B are not independent, Bayes’ Theorem would capture
how strongly the two pieces of information are related.

=======================
Dealing with Rare Words
=======================

If you pick a word that is present in only one of the categories (like “uncle” in Ham)
-> if we saw a message containing the word “uncle,” we would automatically
decide, with 100% confidence, that this message cannot be Spam, regardless of the rest of the message.
-> this can’t be right; we shouldn’t have 100% confidence in anything based on a limited training sample.

One way to address this problem is by using Laplace Smoothing.
-> technique that can be described as a “fudge factor.”

We will eliminate the problem of absent words by augmenting the count of each word by one,
and computing P(Spam contains “xyz”) = (1 + count of Spam containing “xyz”) / (1 + count of Spam)

Essentially, we are making up a fictional message containing every token
	-> rarest tokens will have a very low probability, but not zero.

========================
Combining Multiple Words
========================

There are many words in a message, each of which pushes toward Ham or Spam with various strength.
	-> How can we combine that information -> into a single value?

Breaking Text into Tokens
=========================

“Driving, cant txt.”
-> break the message into a set of words, “driving,” “cant,” and “txt,”
	-> use that representation instead of the original text.

Breaking up a block of text into meaningful elements
	-> TOKENIZATION

* We are transforming each message into a collection of features
	-> each feature being the presence or absence of a particular word.

“I like carrots and hate broccoli” and “I like broccoli and hate carrots” will be broken into
the same set of tokens [and; broccoli; carrots; hate; i; like]
	-> Will therefore be considered identical.
	-> But we are not trying to understand the meaning of sentences here;
	we merely need to identify which words are connected to a particular behavior.

* Rather than argue over whether capitalization or punctuation matters
	-> Settle that question easily with cross-validation later on
	-> Create alternate models for each hypothesis
		-> compare their performance
		-> let the data decide